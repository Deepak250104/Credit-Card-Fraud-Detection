{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5b30f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.models import train_logistic_regression, train_random_forest, train_gradient_boosting, train_lightgbm, evaluate_model\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410081a7",
   "metadata": {},
   "source": [
    "### Loading Processed Data\n",
    "\n",
    "We load the preprocessed and resampled training data and the scaled test data from the files saved in the Feature Engineering notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3a0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load('../data/X_train_resampled.joblib')\n",
    "y_train = joblib.load('../data/y_train_resampled.joblib')\n",
    "X_test = joblib.load('../data/X_test_scaled.joblib')\n",
    "y_test = joblib.load('../data/y_test.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d0056",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation (Initial Baselines)\n",
    "\n",
    "We train and evaluate several baseline classification models to get an initial understanding of their performance on this task. We focus on metrics relevant to imbalanced datasets, as provided by the `evaluate_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49ff7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Confusion Matrix:\n",
      " [[55322  1542]\n",
      " [    8    90]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.06      0.92      0.10        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.95      0.55     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "\n",
      "AUC-ROC: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], shape=(56962,)),\n",
       " array([4.94563032e-03, 4.23430942e-02, 2.27043661e-05, ...,\n",
       "        5.28451843e-04, 1.96395096e-03, 5.25151879e-02], shape=(56962,)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Logistic Regression ---\")\n",
    "lr_model = train_logistic_regression(X_train, y_train)\n",
    "evaluate_model(lr_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10de671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest ---\n",
      "Confusion Matrix:\n",
      " [[56850    14]\n",
      " [   19    79]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.85      0.81      0.83        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.90      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "AUC-ROC: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], shape=(56962,)),\n",
       " array([0.  , 0.  , 0.03, ..., 0.  , 0.  , 0.  ], shape=(56962,)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Random Forest ---\")\n",
    "rf_model = train_random_forest(X_train, y_train)\n",
    "evaluate_model(rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a71ad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient Boosting ---\n",
      "Confusion Matrix:\n",
      " [[56119   745]\n",
      " [    9    89]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.11      0.91      0.19        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.55      0.95      0.59     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n",
      "\n",
      "AUC-ROC: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], shape=(56962,)),\n",
       " array([0.0225519 , 0.00933714, 0.08887581, ..., 0.00340834, 0.01343252,\n",
       "        0.06190657], shape=(56962,)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Gradient Boosting ---\")\n",
    "gb_model = train_gradient_boosting(X_train, y_train)\n",
    "evaluate_model(gb_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd867b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM ---\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Confusion Matrix:\n",
      " [[56796    68]\n",
      " [   13    85]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.56      0.87      0.68        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.78      0.93      0.84     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "AUC-ROC: 0.9463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\OneDrive\\Programming\\Projects\\GitHub\\Credit-Card-Fraud-Detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\deepa\\OneDrive\\Programming\\Projects\\GitHub\\Credit-Card-Fraud-Detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0], shape=(56962,)),\n",
       " array([1.50629968e-04, 2.13502983e-04, 3.49314972e-04, ...,\n",
       "        6.91920436e-05, 5.07660398e-04, 1.81072489e-04], shape=(56962,)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- LightGBM ---\")\n",
    "lgbm_model = train_lightgbm(X_train, y_train)\n",
    "evaluate_model(lgbm_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf1537",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "To potentially improve the performance of our models, we can tune their hyperparameters. This involves searching through a predefined set of parameter values to find the combination that yields the best performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2192bb",
   "metadata": {},
   "source": [
    "Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8c03bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\OneDrive\\Programming\\Projects\\GitHub\\Credit-Card-Fraud-Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\deepa\\OneDrive\\Programming\\Projects\\GitHub\\Credit-Card-Fraud-Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\deepa\\OneDrive\\Programming\\Projects\\GitHub\\Credit-Card-Fraud-Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None, 'C': np.float64(1291.5496650148827)}\n",
      "Tuned Logistic Regression model saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning Logistic Regression...\")\n",
    "param_distributions_lr = {\n",
    "    'C': np.logspace(-4, 4, 10), \n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "random_search_lr = RandomizedSearchCV(LogisticRegression(random_state=42),\n",
    "                                       param_distributions=param_distributions_lr,\n",
    "                                       n_iter=10, cv=3, scoring='roc_auc', n_jobs=1, random_state=42)\n",
    "random_search_lr.fit(X_train, y_train)\n",
    "best_lr = random_search_lr.best_estimator_\n",
    "print(\"Best Logistic Regression parameters:\", random_search_lr.best_params_)\n",
    "joblib.dump(best_lr, '../models/logistic_regression_tuned.joblib')\n",
    "print(\"Tuned Logistic Regression model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaef703",
   "metadata": {},
   "source": [
    "Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97bd9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Random Forest on 10% of training data...\n",
      "Best Random Forest parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'class_weight': 'balanced'}\n",
      "Tuned Random Forest model saved.\n"
     ]
    }
   ],
   "source": [
    "# Sampling 10% of training data for faster hyperparameter tuning\n",
    "np.random.seed(42) \n",
    "sample_indices_rf = np.random.choice(len(X_train), size=int(0.1 * len(X_train)), replace=False)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "X_sample_rf = X_train.iloc[sample_indices_rf]\n",
    "y_sample_rf = y_train.iloc[sample_indices_rf]\n",
    "\n",
    "# Define hyperparameter search space for Random Forest\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': [100, 200],        # number of trees\n",
    "    'max_depth': [None, 10],            # max depth of tree\n",
    "    'min_samples_split': [2, 5],        # min samples needed to split\n",
    "    'min_samples_leaf': [1, 3],         # min samples needed at a leaf\n",
    "    'class_weight': ['balanced']        # handle class imbalance\n",
    "}\n",
    "\n",
    "# Setup RandomizedSearchCV for Random Forest\n",
    "print(\"\\nTuning Random Forest on 10% of training data...\")\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=10,                  # number of random combinations to try\n",
    "    cv=3,                       # 3-fold cross-validation\n",
    "    scoring='roc_auc',           # optimize for ROC-AUC\n",
    "    n_jobs=1,                   # run sequentially to avoid OSError in Jupyter\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "random_search_rf.fit(X_sample_rf, y_sample_rf)\n",
    "\n",
    "# Save the best model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best Random Forest parameters:\", random_search_rf.best_params_)\n",
    "joblib.dump(best_rf, '../models/random_forest_tuned.joblib')\n",
    "print(\"Tuned Random Forest model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14163c3a",
   "metadata": {},
   "source": [
    "Gradient Boosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4787f0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Gradient Boosting on 10% of training data...\n",
      "Best Gradient Boosting parameters: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Retraining best Gradient Boosting model on FULL training data...\n",
      "Final tuned Gradient Boosting model saved.\n"
     ]
    }
   ],
   "source": [
    "# Taking 20% sample of training data for faster tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.9,        # Keep 90% aside, use 10%\n",
    "    stratify=y_train,     # Maintain class distribution\n",
    "    random_state=42       # For reproducibility\n",
    ")\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_distributions_gb = {\n",
    "    'n_estimators': [100, 150],      # Number of trees\n",
    "    'learning_rate': [0.01, 0.1],    # Step size shrinkage\n",
    "    'max_depth': [3, 5],             # Max depth of trees\n",
    "    'min_samples_split': [2, 4],     # Minimum samples to split node\n",
    "    'min_samples_leaf': [1, 2]       # Minimum samples per leaf node\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_distributions=param_distributions_gb,\n",
    "    n_iter=10,              # Try 10 random combinations\n",
    "    cv=3,                   # 3-fold cross validation\n",
    "    scoring='roc_auc',      # Use ROC-AUC for scoring\n",
    "    n_jobs=1,               # run sequentially\n",
    "    random_state=42         # For reproducibility\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning on 10% sample\n",
    "print(\"\\nTuning Gradient Boosting on 10% of training data...\")\n",
    "random_search_gb.fit(X_sample, y_sample)\n",
    "\n",
    "# Get the best model from search\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best Gradient Boosting parameters:\", random_search_gb.best_params_)\n",
    "\n",
    "# Retrain best model on FULL training data\n",
    "print(\"\\nRetraining best Gradient Boosting model on FULL training data...\")\n",
    "best_gb.fit(X_train, y_train)\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(best_gb, '../models/gradient_boosting_tuned.joblib')\n",
    "print(\"Final tuned Gradient Boosting model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfed6f",
   "metadata": {},
   "source": [
    "LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4416361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 151634, number of negative: 151634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 303268, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 227451, number of negative: 227451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 454902, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best LightGBM parameters: {'num_leaves': 31, 'n_estimators': 150, 'learning_rate': 0.1, 'class_weight': None}\n",
      "Tuned LightGBM model saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning LightGBM...\")\n",
    "param_distributions_lgbm = {\n",
    "    'n_estimators': [100, 150],  \n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 40],      \n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "random_search_lgbm = RandomizedSearchCV(LGBMClassifier(random_state=42),\n",
    "                                        param_distributions=param_distributions_lgbm,\n",
    "                                        n_iter=10, cv=3, scoring='roc_auc', n_jobs=1, random_state=42)\n",
    "random_search_lgbm.fit(X_train, y_train)\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "print(\"Best LightGBM parameters:\", random_search_lgbm.best_params_)\n",
    "joblib.dump(best_lgbm, '../models/lightgbm_tuned.joblib')\n",
    "print(\"Tuned LightGBM model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44a44f",
   "metadata": {},
   "source": [
    "# FInding the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fcf5427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.99994430332102)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b84e9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9916568588276671)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b9feef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9998275846550692)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_gb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07caf1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999481089281613)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_lgbm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8868f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model (LightGBM) with AUC: 0.9999 saved as best_model.joblib.\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_auc = 0\n",
    "best_model_name = \"\"\n",
    "\n",
    "if random_search_lr.best_score_ > best_auc:\n",
    "    best_auc = random_search_lr.best_score_\n",
    "    best_model = best_lr\n",
    "    best_model_name = \"Logistic Regression\"\n",
    "\n",
    "if random_search_rf.best_score_ > best_auc:\n",
    "    best_auc = random_search_rf.best_score_\n",
    "    best_model = best_rf\n",
    "    best_model_name = \"Random Forest\"\n",
    "\n",
    "if random_search_gb.best_score_ > best_auc:\n",
    "    best_auc = random_search_gb.best_score_\n",
    "    best_model = best_gb\n",
    "    best_model_name = \"Gradient Boosting\"\n",
    "\n",
    "if random_search_lgbm.best_score_ > best_auc:\n",
    "    best_auc = random_search_lgbm.best_score_\n",
    "    best_model = best_lgbm\n",
    "    best_model_name = \"LightGBM\"\n",
    "\n",
    "if best_model:\n",
    "    joblib.dump(best_model, '../models/best_model.joblib')\n",
    "    print(f\"\\nBest model ({best_model_name}) with AUC: {best_auc:.4f} saved as best_model.joblib.\")\n",
    "else:\n",
    "    print(\"\\nNo best model found during hyperparameter tuning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
